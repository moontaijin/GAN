{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCGAN by myself","provenance":[],"mount_file_id":"1VSBb0_Uk06osw5Idq9CMGkDBupCv7Giz","authorship_tag":"ABX9TyOwRqauV8K/MoOeYVnQcIj3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"UJn9CExTZRK0"},"source":["# **DCGAN** by myself\n","Using vanilla GAN with base code.\n","\n","Change Discriminator from **MLP** to **CNN**"]},{"cell_type":"code","metadata":{"id":"CtLyVWxgYvBg"},"source":["import os\n","import torch\n","import torchvision\n","import cv2\n","import torch.nn as nn\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from matplotlib import pyplot as plt\n","from torch.nn.modules.pooling import MaxPool2d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KyLSfPfdZ58c"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters\n","latent_size = 100\n","hidden_size = 256\n","image_size = 4096\n","num_epochs = 200\n","batch_size = 100\n","root_dir = '/content/drive/MyDrive/MLDL/DCGAN'\n","sample_dir = root_dir + '/samples'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q5XLSd7TZ_Ty","executionInfo":{"status":"ok","timestamp":1632624883238,"user_tz":-540,"elapsed":7,"user":{"displayName":"문태진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12123698542719800493"}},"outputId":"f5df38da-6e4e-4861-b600-628f4f3d47d4"},"source":["# Change current path\n","os.chdir(root_dir)\n","print(os.getcwd())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/MLDL/DCGAN\n"]}]},{"cell_type":"code","metadata":{"id":"csSvlCcKbUHb"},"source":["transform = transforms.Compose([\n","                transforms.Resize(64),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.5],   # 1 for greyscale channels\n","                                     std=[0.5])])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yvIz0amtcbiM"},"source":["# MNIST dataset\n","mnist = torchvision.datasets.MNIST(root='../data/',\n","                                   train=True,\n","                                   transform=transform,\n","                                   download=False)\n","\n","# Data loader\n","data_loader = torch.utils.data.DataLoader(dataset=mnist,\n","                                          batch_size=batch_size, \n","                                          shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"EFcSxxK_cnhM","executionInfo":{"status":"ok","timestamp":1632623085354,"user_tz":-540,"elapsed":671,"user":{"displayName":"문태진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12123698542719800493"}},"outputId":"1a095ecf-0b3d-466f-fddd-b451210020ed"},"source":["# Check Dataset\n","for i in data_loader:\n","  plt.imshow(i[0][0][0,:,:],cmap='gray')\n","  break"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"p8jRGDuocw6X"},"source":["# Generator \n","G = nn.Sequential(\n","    nn.Linear(latent_size, hidden_size),\n","    nn.ReLU(),\n","    nn.Linear(hidden_size, hidden_size),\n","    nn.ReLU(),\n","    nn.Linear(hidden_size, image_size),\n","    nn.Tanh())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"67Nia-fofMnW"},"source":["# Modified Discriminator\n","'''\n"," Discriminator를 단순히 MLP로 구성하는것이 아니라 CNN을 이용해서 feature를 뽑아낸 뒤 그것의 분포를 학습하고\n"," 이를 마지막에 FCN으로 연결해주면 더 좋은 결과가 나오지 않을까 하는 생각에서 해보는 모델\n","'''\n","\n","class Flatten(torch.nn.Module):\n","    def forward(self, x):\n","        return x.view(-1, 1).squeeze(1)\n","\n","D = nn.Sequential(\n","    nn.Conv2d(1,64,kernel_size=4,stride=2,padding=1),\n","    nn.LeakyReLU(0.2, inplace=True),\n","    nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n","    nn.BatchNorm2d(64 * 2),\n","    nn.LeakyReLU(0.2, inplace=True),\n","    nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n","    nn.BatchNorm2d(64 * 4),\n","    nn.LeakyReLU(0.2, inplace=True),\n","    nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n","    nn.BatchNorm2d(64 * 8),\n","    nn.LeakyReLU(0.2, inplace=True),\n","    nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False),\n","    nn.Sigmoid(),\n","    Flatten()\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Bhabj8VfPLD"},"source":["# Device setting\n","D = D.to(device)\n","G = G.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUeXocqyfSkY"},"source":["# Binary cross entropy loss and optimizer\n","criterion = nn.BCELoss()\n","d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n","g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qQdB82cfV5v"},"source":["def denorm(x):\n","    out = (x + 1) / 2\n","    return out.clamp(0, 1)\n","\n","def reset_grad():\n","    d_optimizer.zero_grad()\n","    g_optimizer.zero_grad()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aWFKTZYnfYqC","executionInfo":{"status":"ok","timestamp":1632624891778,"user_tz":-540,"elapsed":4,"user":{"displayName":"문태진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12123698542719800493"}},"outputId":"f567be30-c6a8-4528-f0a6-5d9cc86353b3"},"source":["from torchsummary import summary as summary_\n","summary_(D,(1,64,64),batch_size=100)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [100, 64, 32, 32]           1,088\n","         LeakyReLU-2          [100, 64, 32, 32]               0\n","            Conv2d-3         [100, 128, 16, 16]         131,072\n","       BatchNorm2d-4         [100, 128, 16, 16]             256\n","         LeakyReLU-5         [100, 128, 16, 16]               0\n","            Conv2d-6           [100, 256, 8, 8]         524,288\n","       BatchNorm2d-7           [100, 256, 8, 8]             512\n","         LeakyReLU-8           [100, 256, 8, 8]               0\n","            Conv2d-9           [100, 512, 4, 4]       2,097,152\n","      BatchNorm2d-10           [100, 512, 4, 4]           1,024\n","        LeakyReLU-11           [100, 512, 4, 4]               0\n","           Conv2d-12             [100, 1, 1, 1]           8,192\n","          Sigmoid-13             [100, 1, 1, 1]               0\n","          Flatten-14                      [100]               0\n","================================================================\n","Total params: 2,763,584\n","Trainable params: 2,763,584\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.56\n","Forward/backward pass size (MB): 231.25\n","Params size (MB): 10.54\n","Estimated Total Size (MB): 243.36\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"id":"YxayyZD2faVx","executionInfo":{"status":"error","timestamp":1632625669317,"user_tz":-540,"elapsed":528504,"user":{"displayName":"문태진","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12123698542719800493"}},"outputId":"69d7aff8-1cf4-4bc8-e904-ae0725db0c0f"},"source":["# Start training\n","total_step = len(data_loader)\n","num_epochs = 20\n","for epoch in range(num_epochs):\n","    for i, (images, _) in enumerate(data_loader):\n","        images = images.to(device)\n","        # Create the labels which are later used as input for the BCE loss\n","        real_labels = torch.full((batch_size,), 1,dtype=images.dtype).to(device)\n","        fake_labels = torch.full((batch_size,), 0, dtype=images.dtype).to(device)\n","\n","        # ================================================================== #\n","        #                      Train the discriminator                       #\n","        # ================================================================== #\n","\n","        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n","        # Second term of the loss is always zero since real_labels == 1\n","        outputs = D(images)\n","        d_loss_real = criterion(outputs, real_labels)\n","        real_score = outputs\n","        # Compute BCELoss using fake images\n","        # First term of the loss is always zero since fake_labels == 0\n","        z = torch.randn(batch_size, latent_size).to(device)\n","        fake_images = G(z)\n","        fake_images = torch.reshape(fake_images,(-1,1,64,64))\n","        outputs = D(fake_images)\n","        d_loss_fake = criterion(outputs, fake_labels)\n","        fake_score = outputs\n","\n","        # Backprop and optimize\n","        d_loss = d_loss_real + d_loss_fake\n","        reset_grad()\n","        d_loss.backward()\n","        d_optimizer.step()\n","        \n","        # ================================================================== #\n","        #                        Train the generator                         #\n","        # ================================================================== #\n","\n","        # Compute loss with fake images\n","        z = torch.randn(batch_size, latent_size).to(device)\n","        fake_images = G(z)\n","        #print(fake_images)\n","        fake_images = torch.reshape(fake_images,(-1,1,64,64))\n","        outputs = D(fake_images)\n","        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n","        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n","        g_loss = criterion(outputs, real_labels)\n","        # Backprop and optimize\n","        reset_grad()\n","        g_loss.backward()\n","        g_optimizer.step()\n","        \n","        if (i+1) % 200 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n","                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n","                          real_score.mean().item(), fake_score.mean().item()))\n","    \n","    \n","    # Save real images\n","    if (epoch+1) == 1:\n","        images = images.reshape(images.size(0), 1, 64, 64)\n","        save_image(denorm(images), os.path.join(sample_dir, 'RealImages.png'))\n","    \n","    # Save sampled images\n","    save_image(denorm(fake_images), os.path.join(sample_dir, 'FakeImages-{}.png'.format(epoch+1)))\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [0/20], Step [200/600], d_loss: 0.0000, g_loss: 10.8128, D(x): 1.00, D(G(z)): 0.00\n","Epoch [0/20], Step [400/600], d_loss: 0.0001, g_loss: 10.8499, D(x): 1.00, D(G(z)): 0.00\n","Epoch [0/20], Step [600/600], d_loss: 100.0000, g_loss: 0.0000, D(x): 1.00, D(G(z)): 1.00\n","Epoch [1/20], Step [200/600], d_loss: 100.0000, g_loss: 0.0000, D(x): 1.00, D(G(z)): 1.00\n","Epoch [1/20], Step [400/600], d_loss: 100.0000, g_loss: 0.0000, D(x): 1.00, D(G(z)): 1.00\n","Epoch [1/20], Step [600/600], d_loss: 100.0000, g_loss: 0.0000, D(x): 1.00, D(G(z)): 1.00\n","Epoch [2/20], Step [200/600], d_loss: 100.0000, g_loss: 0.0000, D(x): 1.00, D(G(z)): 1.00\n","Epoch [2/20], Step [400/600], d_loss: 100.0000, g_loss: 0.0000, D(x): 1.00, D(G(z)): 1.00\n","Epoch [2/20], Step [600/600], d_loss: 100.0000, g_loss: 0.0000, D(x): 1.00, D(G(z)): 1.00\n","Epoch [3/20], Step [200/600], d_loss: 100.0000, g_loss: 0.0000, D(x): 1.00, D(G(z)): 1.00\n","Epoch [3/20], Step [400/600], d_loss: 100.0000, g_loss: 0.0000, D(x): 1.00, D(G(z)): 1.00\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-de6243e90acd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Compute loss with fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#print(fake_images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"TrXTp03LgM4x"},"source":[""],"execution_count":null,"outputs":[]}]}